{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] SparkSession 설정\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# [+] SparkSession 객체 생성 및 설정\n",
    "spark = SparkSession.builder.master('local').appName('mllib-turorial').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib 예제: Logistic Regression\n",
    "+ ```pyspark.ml.linalg```: 선형대수(linear algebra) 관련 패키지\n",
    "    + ```Vectors```: 벡터 데이터 타입\n",
    "+ ```pyspark.ml.classification```: 분류 모델 관련 패키지\n",
    "    + ```LogisticRegression```: 가장 기본적인 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors, LogisticRegression 임포트\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    데이터 정의: 레이블과 특징\n",
    "    - dense(): 모든 값을 저장하는 벡터 생성\n",
    "    - sparse(): 0이 아닌 값만 저장하는 벡터 생성\n",
    "\"\"\"\n",
    "\n",
    "train_data = [\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),     # (레이블(정답), 특징값(feature))\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5])),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] 스키마 정의\n",
    "schema = ['label', 'features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] DataFrame 생성\n",
    "train_df = spark.createDataFrame(train_data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|label|      features|\n",
      "+-----+--------------+\n",
      "|  1.0| [0.0,1.1,0.1]|\n",
      "|  0.0|[2.0,1.0,-1.0]|\n",
      "|  0.0| [2.0,1.3,1.0]|\n",
      "|  1.0|[0.0,1.2,-0.5]|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [+] DataFrame 출력\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    LogisticRegression (Estimator) 생성\n",
    "    - maxIter: 최대 학습 횟수 매개변수\n",
    "    - regParam: 정규화 매개변수\n",
    "\"\"\"\n",
    "\n",
    "lr = LogisticRegression(maxIter=30, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_fd54c0f51c70, numClasses=2, numFeatures=3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 테스트\n",
    "test_data = [\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5])),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] DataFrame 생성\n",
    "test_df = spark.createDataFrame(test_data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0|[-1.0,1.5,1.3]|[-6.2435550918400...|[0.00193916823498...|       1.0|\n",
      "|  0.0|[3.0,2.0,-0.1]|[5.45228608726759...|[0.99573180142693...|       0.0|\n",
      "|  1.0|[0.0,2.2,-1.5]|[-4.4104172202339...|[0.01200425500655...|       1.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측결과 출력\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression 파이프라인 구현하기\n",
    "+ 예측 문제: Spark에 대한 텍스트 인지 아닌지를 분류\n",
    "+ ```Pipeline```: 머신러닝 과정에 대한 파이프라인 정의\n",
    "+ ```HashingTF```: 컬럼을 용어빈도(term-frequency) 컬럼으로 변환하는 Transformer\n",
    "+ ```Tokenizer```: 컬럼의 텍스트를 여러 개의 단어로 분할하는 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에 대한 DataFrame 생성\n",
    "train_df = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),   # true 1\n",
    "    (1, \"b d\", 0.0),               # false 0\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])    # schema 바로 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    머신러닝 파이프라인 과정: Tokenizer -> HashingTF -> LinearRegression\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizer 객체 생성\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+--------------------+\n",
      "| id|            text|label|               words|\n",
      "+---+----------------+-----+--------------------+\n",
      "|  0| a b c d e spark|  1.0|[a, b, c, d, e, s...|\n",
      "|  1|             b d|  0.0|              [b, d]|\n",
      "|  2|     spark f g h|  1.0|    [spark, f, g, h]|\n",
      "|  3|hadoop mapreduce|  0.0| [hadoop, mapreduce]|\n",
      "+---+----------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.transform(train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF 객체 생성\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    [+] LogisticRegression 객체 생성\n",
    "    - maxIter = 10\n",
    "    - regParam = 0.001\n",
    "\"\"\"\n",
    "\n",
    "lr = LogisticRegression(maxIter = 30, regParam = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] Pipeline 객체 생성\n",
    "pipeline = Pipeline(stages = [tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] Pipeline 실행 -> 모델 생성\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 DataFrame 생성\n",
    "test_df = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [+] 테스트 데이터에 대한 예측\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|       spark i j k|    [spark, i, j, k]|(262144,[19036,68...|[0.37446592544992...|[0.59253766108921...|       0.0|\n",
      "|  5|             l m n|           [l, m, n]|(262144,[1303,526...|[2.83313663883221...|[0.94444041965042...|       0.0|\n",
      "|  6|spark hadoop spark|[spark, hadoop, s...|(262144,[173558,1...|[-1.1622095936644...|[0.23826602233961...|       1.0|\n",
      "|  7|     apache hadoop|    [apache, hadoop]|(262144,[68303,19...|[3.75513183310007...|[0.97713755512847...|       0.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
